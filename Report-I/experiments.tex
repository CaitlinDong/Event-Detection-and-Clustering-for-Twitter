\section{\uppercase{Experiments}}
Parallel to our literature survey on topic detection techniques for Twitter, we have also conducted preliminary experiments on a Twitter dataset having 1.6 million tweets. We processed the available data which was in CSV format into a format that can be fed into the LDA program. Next, we ran LDA on this dataset, with each tweet as a document, without any tweet pooling or aggregation. The topic clusters returned by LDA were noisy, with hashtags, usernames, slangs and keywords from all domains (stop words, mood-related, entities, event related etc.) cluttered together. Another reason for these impure clusters was the fact that users use different and often incorrect, spellings for the same words, resulting in potential assignment of same terms to different topics.

Our preliminary experiments have helped us gain better insight into the topic modeling paradigm, and the issues pertinent to its naive use over Twitter dataset. We are in the process of perfoming preprocessing on Twitter dataset to reduce noise, and experimenting with different variations of LDA to come up with a suitable model which generates meaningful and pure topic clusters.
